{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>Chapter VI<br><br>\n",
    "Method for unconstrain problems<br><br>\n",
    "\n",
    "$\\qquad$Although many real-life optimization problems involes constraints, the methods used to solved them are bases on numerical methods for unconstrained problem. So this chapter will consider numerical methods that are use to try to find a local minimizer $x^*$ of<br> Min $f(\\bar{x}$), $\\bar{x} \\in \\mathbb{R}^2$ and a minimizer $x^*$ of the unconstrained problem must be a stationary point with $\\nabla f(x^*) = 0$\n",
    "\n",
    "An example of the constraint problem is solved by methods of un constrained problem\n",
    "\n",
    "Ex.\n",
    "$$\n",
    "\\begin{align*}\n",
    "Min \\:  f(\\bar{x}) = -x_1x_2 & \\qquad eqn1 \\\\\n",
    "\\\\\n",
    "Subj \\: to \\qquad x_1+x_2-2=0 & \\qquad eqn2 \\\\\n",
    "\\\\\n",
    "From \\: eq2 \\qquad x_1+x_2-2 = 0 & \\qquad then \\qquad x_1=2-x_2 \\qquad \\:eqn3 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "substituting eqn3 into eqn1 ,then\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x_2) &= -x_1x_2\\\\\n",
    "&= -(2-x_2)x_2\\\\\n",
    "&= -2x_2+x_2^2\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Find a minimizer of $f(x_2)$;\n",
    "$$\n",
    "\\begin{align*}\n",
    "f'(x_2) &= -2+2x_2 \\: = 0\\;\\rightarrow\\;x_2\\:=\\:1\\:\\;and\\:\\;x_1=1\\\\\n",
    "\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, a minimizer of $f(\\bar{x})$ is $x$* = $\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "with $f(x^*)$ , and $f''(x^*)$ = $2\\:>\\:0$\n",
    "\n",
    "numerical algorithms require a starting point oe initial point $x^0$ , then to generate a sequence of point {$x^{(k)}$} which progress steadily towards a neighbourhood of a local minimizer $x^*$ and the sequence of iteration{$x^{(k)}$}  is $f(x^{(k+1)}) < f(x^{(k)})$ which is descent direction and be called a descent method\n",
    "\n",
    "This ensures that the function value is reduced on each iteration, but is not enough to guaruntee convergence to a local minimizer . So at a point $x^{(k)}$, we will determine the step $\\alpha^{(k)}>0$ in the direction $d^{(k)}$ based on a local model of the objective function about the point $x^{(k)}$.\n",
    "\n",
    "the new point is \n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{(k+1)}\\: = x^{(k)}+\\alpha^{(k)}d^{(k)}\\\\\n",
    "\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "when $d^{(k)}$ is a descent direction for $f(\\bar{x})$ at $x^{(k)}$ such that $\\bar{d}^T$$^{(k)}$$\\nabla f(x^{(k)}) < 0$\n",
    "\n",
    "It means that the directional derivertive of $f(\\bar{x})$ in the direction $d^{(k)}$ decrease such that $\\bar{d}^T\\nabla f = f'(\\bar{x}+\\alpha\\bar{d})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.1 Line search methods<br>\n",
    "\n",
    "$\\qquad$Many of the better numerical method for solving minimization problem fit within the framework of line search method the process of determining a minimizer on a given line is called line search\n",
    "\n",
    "Algorithm of line search\n",
    "Suppose that we are at a point $x^{(k)}$ then the $k^{th}$ iteration is as follows\n",
    "\n",
    "1.Test for convergence , if the condition for convergence are satisfied, the algorithm terming with $x^{(k)}$ on the solution,\n",
    "\n",
    "2.determine or search a descent direction\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "d^{(k)}\\: = -\\nabla f\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "3.Find $\\alpha^{(k)}$ to minimize $f(x^{(k)}+\\alpha^{(k)}d^{(k)})$ with respect to $\\alpha \\leq 0 $, is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell_k(\\alpha) \\: & = f(x^{(k)}+\\alpha^{(k)}d^{(k)})\\\\\n",
    "\\ell'_k(\\alpha) \\: & = \\bar{d}^T\\nabla f = 0\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "  The step $\\alpha^{(k)}$ is chosen by exactly or approximate minimizing the function value $\\ell_k(\\alpha) \\: = f(x^{(k)}+\\alpha^{(k)}d^{(k)})$ which the exact minimizer satisfied\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell'_k(\\alpha) \\: & = \\nabla f^T (x^{(k)}+\\alpha^{(k)}d^{(k)})\\bar{d}^{(k)} \\:=\\: 0\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4.set $x^{(k+1)}\\: = x^{(k)}+\\alpha^{(k)}d^{(k)}$\n",
    "\n",
    "if $d^{(k)}$ is a descent direction for $f(\\bar{x})$ at $x^{(k)}$ then a line search produces a step $\\alpha^k > 0$ such that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x^{(k+1)}) \\: < f(x^{(k)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. (Exact line search)\n",
    "\n",
    "Let $f(\\bar{x}) = x^2_1+10x^2_2$ (Quadratic function)\n",
    "\n",
    "At initial pt,\n",
    "\n",
    "$x^{(1)}$ = $\\begin{bmatrix} 0 \\\\ \\frac{1}{10} \\end{bmatrix}$ and $-\\nabla f= -g(\\bar{x}) = \\begin{bmatrix} 2x_1 \\\\ 20x_2 \\end{bmatrix}  $ and $d^{(1)} = \\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix}\\:=\\:-g(x^{(1)})$\n",
    "\n",
    "$x^{(1)}+\\alpha^{(1)}d^{(1)}$=$\\begin{bmatrix} 0 \\\\ \\frac{1}{10} \\end{bmatrix}$+$\\alpha\\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix}$=$\\begin{bmatrix} 0 \\\\ \\frac{1}{10}-2\\alpha \\end{bmatrix}$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla f^T(x^{(1)}+\\alpha^{(1)}d^{(1)})\\bar{d}^{(1)} & =\n",
    "\\begin{bmatrix} 0 & 20(\\frac{1}{10}-2\\alpha) \\end{bmatrix} \n",
    "\\begin{bmatrix} 0 \\\\ - 2 \\end{bmatrix}\\\\\n",
    "& =-20(\\frac{1}{10}-2\\alpha)=0 \\\\\n",
    "\\alpha &=\\:\\frac{1}{20} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac {d}{d\\alpha}f(x^{(1)}+\\alpha^{(1)}d^{(1)})&=\\frac {d}{d\\alpha}f\\begin{bmatrix} 0 \\\\ \\frac{1}{10}-2\\alpha \\end{bmatrix}\\\\\n",
    "&=\\:\\frac {d}{d\\alpha}[0+10(\\frac{1}{10}-2\\alpha)^2]\\\\\n",
    "&=\\:-20(\\frac{1}{10}-2\\alpha)\\:=\\:0\\\\\n",
    "\\alpha &= \\frac{1}{20}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore,$\\nabla f^T(x^{(1)}+\\alpha^{(1)}d^{(1)})d = \\frac {d}{d\\alpha}f(x^{(1)}+\\alpha^{(1)}d^{(1)})$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{(2)}&=\\:x^{(1)}+\\alpha^{(1)}d^{(1)}\\\\\n",
    "&=\\begin{bmatrix} 0 \\\\ \\frac{1}{10}\\end{bmatrix}+\\frac{1}{20}\\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix}\\\\\n",
    "&=\\begin{bmatrix} 0 \\\\ 0\\end{bmatrix}\\; which\\:is\\:a\\:minimizer\\:of\\:f(\\bar{x})\\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.1.1 Convergence test<br>\n",
    "\n",
    "$\\qquad$Numerical method would terminate when it was within a preseribed distance of the solution point or the optimal value.\n",
    "\n",
    "The iteration would be stopped when either or both of $\\lVert x^{(k)} - x^* \\rVert \\leq \\epsilon_x$ and $\\lVert fx^{(k)} - f(x^*) \\rVert \\leq \\epsilon_f$ are satisfied\n",
    "\n",
    "Where $\\epsilon_x$ and $\\epsilon_f$ are small positive tolerance, the desized accuracy in the solution point and the objective value respectively\n",
    "\n",
    "For example using $\\epsilon_x=0.5x10^{(-5)}$ would guarantee that each component has an absolute error less than $0.5x10^{(-5)}$.\n",
    "\n",
    "Unfortunately , this convergence test are not practical as $x^*$ and $f^*$ are generally not known.\n",
    "\n",
    "There are alternative test if $f^{(k)}\\rightarrow f^*$, then $|f^{(k)}-f^{(k+1)}|\\rightarrow 0$ as adescent method has $f^{(k+1)}\\leq f^{(k)}$\n",
    "\n",
    "The converse does not hold, so it is possible for $|f^{(k)}-f^{(k+1)}|\\rightarrow 0$ without $f^{(u)}\\rightarrow f^*$.\n",
    "\n",
    "similarly, if $x^{(k)}\\rightarrow x^*$ then $\\lVert x^{(k+1)-x^{(k)}} \\rVert \\rightarrow 0$ but $\\lVert x^{(k+1)-x^{(k)}} \\rVert \\rightarrow 0$ does not neccessarily imply $x^{(k)}\\rightarrow x^*$.\n",
    "\n",
    "As $x^*$ is not know , so more of the convergence condition\n",
    "$$\n",
    "\\begin{align*}\n",
    "|f^{(k)}-f^{(k)+1}| \\leq \\epsilon_f\\:,\\\\\n",
    "\\lVert x^{(k)}-x^{(k)+1} \\rVert \\leq \\epsilon_x\\:,\\\\\n",
    "\\lVert g^{(k)} \\rVert \\leq \\epsilon_g\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "are typically used to the algorithm terminate\n",
    "\n",
    "For $\\lVert g^{(k)} \\rVert \\leq \\epsilon_g$ is based on the neccessary condition that $\\nabla f(x^*) = g(x^*) = 0$. However a stationary point may also corresspond to a local max or a saddle point. Then this condition can ensure the method to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.1.2 Approximately line search<br>\n",
    "\n",
    "$\\qquad$Let $\\ell_k(\\alpha) \\: = f(x^{(k)}+\\alpha^{(k)}d^{(k)})$ where $x^{(k)}\\:and \\: d^{(k)}$ are fixed. Assume $d^{(k)}$ is a descent direction , we get $\\ell'_k(0) < 0$\n",
    "\n",
    "Now we wish to find a local minimizer $\\alpha^*$ of $\\ell_k(\\alpha)$ over $\\alpha > 0$\n",
    "\n",
    "As we noted before in practice , it is too difficult to calculate an exact minimizer of $\\alpha^{(k)}$ . So we have be satisfied will on approximate minimizer $\\alpha^{(k)}$.\n",
    "\n",
    "Two of the most useful approximate line search condition on $\\alpha^{(k)}$ are\n",
    "\n",
    "1. Sufficient function decrease\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell_k(\\alpha) \\: & = \\ell_k(0)+\\rho\\alpha\\ell'_k(0)\n",
    "\\end{align*}\n",
    "$$\n",
    "2. Sufficient slope improvement\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lVert \\ell'_k(\\alpha)  \\rVert \\: & \\leq -\\delta\\ell'_k(0)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note 1 The $\\ell_k(\\alpha) \\: = \\ell_k(0)+\\rho\\alpha\\ell'_k(0)$ is the straight line passing through $(0,\\ell'_k(0))$ with slope $\\rho\\ell'_k(0)$ at $\\alpha$ = 0 lines<br>\n",
    "where $\\rho$ = 0 and $0<\\rho<1$ have been oket\n",
    "\n",
    "Note also that simply require that\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x^{(k)}+\\alpha^{(k)}d^{(k)}) \\: & =\\ell_k(\\alpha^{(k)})<\\ell_k(0)=f(x^{(k)})\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "is not good enough to forec the convergence of line search method.\n",
    "\n",
    "As $d^{(k)}$ is a descent direction there exists an interval $[0,\\alpha_f]$ where $\\alpha_f>0$ such that $\\ell_k(\\alpha) \\: = \\ell_k(0)+\\rho\\alpha\\ell'_k(0)$ is satisfied for any $\\alpha^{(k)}\\in[0,\\alpha_f]$\n",
    "\n",
    "Note 2 The second condition $\\lVert \\ell'_k(\\alpha)  \\rVert \\: \\leq -\\delta\\ell'_k(0)$ determine an interval $[\\xi_1,\\xi_2]$ about the stationary point\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lVert \\ell'_k(\\alpha)  \\rVert \\: & \\leq -\\delta\\ell'_k(0) \\qquad or\\\\\n",
    "\\delta\\ell'_k(0) \\: & \\leq \\ell'_k(\\alpha) \\: \\leq -\\delta\\ell'_k(0)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Any point $\\alpha^{(k)}$ which satisfied both condition 1 and 2 is acceptable on an approximate minimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. (approximate line search)\n",
    "\n",
    "Consider the function\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\bar{x}) \\: & = 3x_1^4-4x_1^3-12x_1^2+(x_2-1)^2+12\n",
    "\\end{align*}\n",
    "$$\n",
    " \n",
    "Let $x^{(k)} = v $ and $d^{(k)} = \\begin{bmatrix} \\frac{1}{2} \\\\ 0 \\end{bmatrix} $ be the descent direction then $x^{(k)}+\\alpha_kd^{(k)} = \\begin{bmatrix} 1+\\frac{\\alpha}{2} \\\\ 0 \\end{bmatrix}$ and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell_k(\\alpha_k) \\: & =  3(1+\\frac{\\alpha}{2})^4-4(1+\\frac{\\alpha}{2})^3-12(1+\\frac{\\alpha}{2})^2+13\\\\\n",
    "\\: & =  \\frac{3}{16}\\alpha^4+\\alpha^3-\\frac{3}{2}\\alpha^2-12\\alpha\\\\\n",
    "\\ell_k'(\\alpha_k) \\: & =  \\frac{3}{4}\\alpha^3+3\\alpha^2-3\\alpha-12\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "the exact minimizer must satisfy $\\ell_k'=0$ which has solution $\\alpha_k=-4,-2,2.$\n",
    "\n",
    "As $d_k$ is a descent direction , $\\alpha_k$ must be positive, so $\\alpha_k^*=2$ substitution show that $\\ell_k(\\alpha^*_k)=-19$ and $\\ell_k''(\\alpha_k^*)=18>0$, so $\\alpha_k^*=2$ is at least a local minimizer of $\\ell_k(\\alpha_k)$\n",
    "\n",
    "As $\\ell_k(0) = f(x_k)=0 $ and $\\ell_k'=\\nabla f^T(x_k)d_k =-12,$\n",
    "\n",
    "the approximate line search $i$ and $ii$ become\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "[\\ell_k(\\alpha) \\: & \\leq \\ell_k(0)+\\alpha\\rho\\ell_k'(0)]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{3}{16}\\alpha^4+\\alpha^3-\\frac{3}{2}\\alpha^2-12\\alpha\\:\\leq\\:-12\\alpha\\rho\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "[|\\ell_k'(\\alpha)| \\: & \\leq -\\delta\\ell_k(0)]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{3}{4}\\alpha^3+3\\alpha^2-3\\alpha-12\\leq\\:12\\delta\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.2 The method of steepest descent (the gradient method)<br>\n",
    "\n",
    "$\\qquad$One of the oldest and most widely known method for minimizing of a function of several variables is the method steepest descant which often refered to an the gradient \n",
    "\n",
    "A basic property of the gradient is that a point $\\bar{x}$ , $\\bar{d} \\:=\\: -g(\\bar{x}) = \\nabla f(\\bar{x})$, is the direction in which the function decreases most rapidly at $\\bar{x}$ , and recall the direction $\\bar{d}$ is a decent direction. If $\\bar{d}^Tg(\\bar{x})<0$ for $\\bar{d} \\:=\\: -g(\\bar{x}) = \\nabla f(\\bar{x})$, then $\\bar{d}^Tg(\\bar{x})\\:=\\:-g^T(\\bar{x})g(\\bar{x})\\:=\\:-\\lVert g(\\bar{x})  \\rVert^2\\:<0$\n",
    "\n",
    "The method of steepest descent is defined by the following iteration algorithm\n",
    "\n",
    "Algorithm Steepest descent\n",
    "Given starting point  $x_0$ and a tolerance $\\epsilon_g>0$\n",
    "\n",
    "1. Set $k\\:=\\:0$\n",
    "\n",
    "2. if $\\lVert g^{(k)} \\rVert \\leq \\epsilon_g$ stop\n",
    "\n",
    "3. Let $\\bar{d}^{(k)} = -g \\equiv $ steepest descent direction\n",
    "\n",
    "4. Find $\\alpha_k$ to minimize of $f(\\bar{x}^{(k)}+\\alpha_k\\bar{d}^{(k)})$\n",
    "\n",
    "5. set $x^{(k)}\\:=\\:x^{(k)}+\\alpha_k\\bar{d}^{(k)}$\n",
    "\n",
    "6. set $k=k+1$ and go to 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex.<br>\n",
    "Let $f(\\bar{x})=x_1^2-4x_1+5x_2^2+30x_2+50$ and a starting point $x_0 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$ . find an analytic solution of unconstrained problem\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "The\\:gradient\\:\\nabla f(\\bar{x}) = \n",
    "\\begin{bmatrix}\n",
    "    2x_1-4 \\\\\n",
    "    10x_2+30 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "and\\:Hessian\\:\\nabla^2 f(\\bar{x}) =\n",
    "\\begin{bmatrix}\n",
    "    2 & 0\\\\\n",
    "    0 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Since $\\nabla^2f$ is constant and positive definite matrix for all $\\bar{x}\\in \\mathbb{R}^2, f{(\\bar{x})}$ is the strictly convex function \n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{equation*}\n",
    "Let \\;\\;\\nabla f(\\bar{x})=0\\:,\\:then\\\\\n",
    "\\begin{bmatrix}\n",
    "    2x_1-4 \\\\\n",
    "    10x_2+30 \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "    2\\\\\n",
    "    -3\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Hence $x^* = \n",
    "\\begin{bmatrix}\n",
    "    2\\\\\n",
    "    -3\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "is the unique global minimizer.\n",
    "\n",
    "Solving by the steepest descent method\n",
    "\n",
    "Iteration 1 , $x^0=\n",
    "\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    -2\\\\\n",
    "\\end{bmatrix}$\n",
    "then\n",
    "$g_0$=$\\begin{bmatrix} -2\\\\ 10 \\end{bmatrix}$, so the steepest direction at $x^0$ is\n",
    "\n",
    "$d^0 = -g^0$ = $-\\begin{bmatrix} -2 \\\\ 10 \\end{bmatrix}$ = $\\begin{bmatrix} 2 \\\\ -10 \\end{bmatrix}$\n",
    "\n",
    "Line search to find $\\alpha_0 \\geq 0 $ to minimize $f(x_0+\\alpha_0\\bar{d}_0)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Solve \\qquad & g^T(x_0+\\alpha_0\\bar{d}_0)\\bar{d}_0  = 0 \\\\\n",
    "where \\qquad x^0+\\alpha d^0 & = \n",
    "\\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}+\\alpha\n",
    "\\begin{bmatrix} 2 \\\\ -10\\end{bmatrix}=\n",
    "\\begin{bmatrix} 1+2\\alpha \\\\ -2-10\\alpha\\end{bmatrix}\\\\\n",
    "g^T(x_0+\\alpha_0\\bar{d}_0)\\bar{d}_0 \\: \n",
    "& = \\begin{bmatrix} 2(1+2\\alpha)-4 \\\\ 10(-2-10\\alpha)+30 \\end{bmatrix}^T \n",
    "\\begin{bmatrix} 2 \\\\ -10 \\end{bmatrix} =0 \\\\\n",
    "& =\\begin{bmatrix} 4\\alpha)-2 \\ -100\\alpha+10 \\end{bmatrix}^T\n",
    "\\begin{bmatrix} 2 \\\\ -10 \\end{bmatrix} =0 \\\\\n",
    "& =8\\alpha-4+1000\\alpha-100 =0 \\\\\n",
    "& =1008\\alpha_0=1040 \\\\\n",
    "& =\\alpha_0=\\frac{104}{1008}=\\frac{13}{126}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence $\\alpha_0 = \\frac{13}{126} > 0 $ is minimizer\n",
    "\n",
    "New point : $x^{(1)} = x^0+\\alpha_0d^0$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}+\\frac{13}{126}\\begin{bmatrix} 2 \\\\ -10 \\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix} \\frac{76}{63} \\\\ \\frac{-191}{63} \\end{bmatrix}=\n",
    "\\begin{bmatrix} 1.20234 \\\\ -3.03170 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Iteration 2\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix} \\frac{76}{63} \\\\ \\frac{-191}{63} \\end{bmatrix},g^{(1)} =\n",
    "\\begin{bmatrix} 2\\frac{76}{63}-4 \\\\ 10\\frac{-191}{63}+30 \\end{bmatrix} \\\\\n",
    "and \\qquad \\bar{d}^{(1)}=-g^{(1)}=-\\nabla(x_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The attractions of the steepest descent method are its simplicity and nice global convergence properties the deepest decent method(with exact or approximate line search) is guarunteed to converge to a stationary point under the condition of convex function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{Theorem}\\;\\;$(Rate of convergence of the steepest descant method)<br>\n",
    "\n",
    "Let $x^*$ be a minimizer of $f(x)$ with $G^*= G(x^*) = \\nabla^2f(x^*)$ positive definite. Suppose that $G*$ has eigen values $0<\\lambda^*_1<\\lambda^*_2<...<\\lambda^*_n$. If {$x_k$} is a sequence of point generated by the steepest descent method with exact line search that converge to $x^*$, then it has linear convergence with converge ratio\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\n",
    "\\frac{(\\lambda^*_n-\\lambda^*_1)}{(\\lambda^*_n+\\lambda^*_1)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note For a convex quadratic function , $G(x^*) = const.$ and positive definite\n",
    "\n",
    "$$\n",
    "\\Large\\begin{align*}\n",
    "\\frac{\\lVert x_{k+1}-x^* \\rVert}{\\lVert x_k-x^* \\rVert} \\leq \\frac{\\lambda_n-\\lambda_1}{\\lambda_n+\\lambda_1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "There are many special cases where the steepest descent method works very well on a quadratic function If $G = \\nabla^2f(\\bar{x}) = \\eta I for \\:\\eta > 0$ and $I$ identity matrix. then\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "G = \n",
    "\\begin{bmatrix}\n",
    "    \\eta & \\dots  & 0 \\\\\n",
    "    \\vdots & \\eta & \\vdots \\\\\n",
    "    0 & \\dots  & \\eta\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Hessian is positive scalars multiple of the identity matrix the $\\lambda_i = \\eta $ for $i = 1,2,...,n ,$\n",
    "so $\\lambda_1 = \\lambda_n = \\eta $ and we get superlinear convergence \n",
    "\n",
    "i.e.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lim_{k \\to \\infty}\\frac{\\lVert x_{k+1}-x^* \\rVert}{\\lVert x_k-x^* \\rVert} &\\leq \\frac{\\lambda_n-\\lambda_1}{\\lambda_n+\\lambda_1} \\qquad for \\;\\lambda_1=\\lambda_n\\\\\n",
    "&\\leq \\frac{1-\\frac{\\lambda_1}{\\lambda_n}}{1+\\frac{\\lambda_1}{\\lambda_n}}=0\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For example :  Let the quadratic function\n",
    "$\n",
    "\\begin{align*}\n",
    "f(\\bar{x}) = x_1^2+x_2^2+x_3^2+...+x_n^2\n",
    "\\end{align*}\n",
    "$\n",
    ",then the rate of convergence has a supperlinear\n",
    "\n",
    "\n",
    "Note that the convergence rate actually depend only the ratio $\\frac{\\lambda_n}{\\lambda_1}$ of the largest and the smallest eigen values\n",
    "\n",
    "i.e. $\n",
    "\\begin{align*}\n",
    "\\frac{\\lVert x_{k+1}-x^* \\rVert}{\\lVert x_k-x^* \\rVert} \\leq \\frac{1-\\frac{\\lambda_1}{\\lambda_n}}{1+\\frac{\\lambda_1}{\\lambda_n}}\\\\\n",
    "\\end{align*}\n",
    "$\n",
    "if a matrix A has eigen values\n",
    "\n",
    "$0 \\leq \\lambda_1 \\leq \\lambda_2\\: ...\\:\\leq \\lambda_n $then the condition number $k(A)$ of the matrix A is given by $k(A)=\\frac{|\\lambda_n|}{|\\lambda_1|} \\geq 1$\n",
    "\n",
    "If $G(x^*)=G^*$ is positive then\n",
    "\n",
    "$k(G^*) = \\frac{\\lambda_n}{\\lambda_1}$ and\n",
    "\n",
    "$\\beta = \\frac{\\lambda_n-\\lambda_1}{\\lambda_n+\\lambda_1} = \\frac{\\frac{\\lambda_n}{\\lambda_1}-1}{\\frac{\\lambda_n}{\\lambda_1}+1} = \\frac{k(G^*)-1}{k(G^*)+1}$\n",
    "\n",
    "If $k(G^*) \\rightarrow 1 $ then $\\beta \\rightarrow0$. the rate of convergence has rapidly linear convergence or super linear.\n",
    "\n",
    "If $k(G^*) \\rightarrow 1 $ then $\\beta \\approx 1$. the rate of convergence has slow linear convergence.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Consider the strictly convex quadratic function $f(\\bar{x})=x_1^2-4x_1+5x_2^2+30x_2+50$ and a starting point $x_1 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$ .Estimate the least number of iterations that would be sufficient to reach a point within a distance of $0.5x10^{(-5)}\\:of\\:x^*$ strating for $x_1$. Using the steepest descent method with exact line search.\n",
    "\n",
    "The hessian $\n",
    "\\begin{equation*}\n",
    "G\\:=\n",
    "\\begin{bmatrix}\n",
    "    2 & 0\\\\\n",
    "    0 & 10 \\\\\n",
    "\\end{bmatrix}\\;\n",
    "and\\; x^*=\n",
    "\\begin{bmatrix}\n",
    "    2\\\\\n",
    "    -3\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$\n",
    "as before\n",
    "\n",
    "The eigen value are $\\lambda_1 = 2$ and $\\lambda_2 = 10$ , so the condition number of the matrix $G$ is $k(G) = \\frac{\\lambda_n}{\\lambda_1}=\\frac{10}{2}=5$\n",
    "\n",
    "For the strictly convex function\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\lVert x^{k+1}-x^* \\rVert}{\\lVert x^k-x^* \\rVert} &\\leq \\frac{k(G)-1}{k(G)+1}\\:=\\frac{5-1}{5+1}=\\frac{2}{3}\\\\\n",
    "\\lVert x^{k+1}-x^* \\rVert &\\leq \\frac{2}{3}\\lVert x^k-x^* \\rVert \\qquad for \\: each\\:k\\\\\n",
    "&\\leq \\frac{2}{3}\\left[\\frac{2}{3}\\lVert x^{k-1}-x^* \\rVert\\right]\\\\\n",
    "&\\:\\vdots\\\\\n",
    "&\\leq \\frac{2}{3}^k\\lVert x^{(1)}-x^* \\rVert\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "After k iteration , we reach $x_{(k+1)}$ starting from $x_1$ and $\\lVert x^{k+1}-x^* \\rVert \\leq (\\frac{2}{3})^k\\lVert x^{(1)}-x^* \\rVert$ ,\n",
    "\n",
    "Where $\\lVert x^{(1)}-x^* \\rVert = \\lVert \\begin{bmatrix} 1\\\\ -2\\\\ \\end{bmatrix}-\\begin{bmatrix} 2\\\\ -3\\\\ \\end{bmatrix}\\rVert = \\lVert \\begin{bmatrix} 1\\\\ -1\\\\ \\end{bmatrix}\\rVert=\\sqrt{2} $\n",
    "\n",
    "After K iteration\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lVert x^{k+1}-x^* \\rVert \\leq \\frac{2}{3}^k\\sqrt{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We need to find  k so that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lVert x^{k+1}-x^* \\rVert \\leq 0.5x10^{-5}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "It is sufficient if we find k, so that\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\frac{2}{3})^k\\sqrt{2} &\\leq 0.5x10^{-5}\\\\\n",
    "(\\frac{2}{3})^k &\\leq \\frac{0.5x10^{-5}}{\\sqrt{2}}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "take logarithm on the both sides\n",
    "$$\n",
    "\\begin{align*}\n",
    "k ln (\\frac{2}{3}) &\\leq ln\\frac{\\frac{0.5x10^{-5}}{\\sqrt{2}}}{\\frac{2}{3}}=30.4\\approx 31\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we need at least 31 iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.3 Newton's method<br>\n",
    "\n",
    "$\\qquad$Newton's method is based in the quadratic mode of the objective function obtained from a second order Taylor series expansion.\n",
    "\n",
    " At iteration $x_k$,\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x^{(k)}+d)\\:\\approx\\:q^{(k)}(d)\\:=f(x^{(k)})+d^T\\nabla f(x^{(k)})+\\frac{1}{2}d^T\\nabla^2f(x^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x^{(k)}+d)\\:\\approx\\:q^{(k)}(d)=\\:f^{(k)}+d^T g^{(k)}+\\frac{1}{2}d^T G^{(k)}d.\n",
    "\\end{align*}\n",
    "$$\n",
    "The idea is to choose a step $d^{(k)}$ as a minimize of $q^{(k)}(d)$. then $d^{(k)}$ is the solution of\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla q^{(k)}(d)\\:=\\:G^{(k)}d+g^{(k)}\\:=0\n",
    "\\end{align*}\n",
    "$$\n",
    "If $\\nabla^2 q^{(k)}(d)\\:=\\:G^{(k)}$ is positive definite then $d_k$ is the minimizer of $q^{(k)}(d)$\n",
    "\n",
    "Note that the approximate model function $q_k(d)$ only has an unique minimizer if $G^{(k)}$ is positive \n",
    "\n",
    "the direction $d^{(k)}$ that satisfies the equation\n",
    "$$\n",
    "\\begin{align*}\n",
    "G^{(k)}d^{(k)}\\:=\\:-g^{(k)}\n",
    "\\end{align*}\n",
    "$$\n",
    "is known as the Newton direction for $f(\\overline{x})$ at $x^{(k)}$\n",
    "\n",
    "Algorithm (Newton's method)\n",
    "Given a starting pt. $x^{(0)}$, set $k=0$\n",
    "\n",
    "1. Calculate $g^{(k)}$ and $G^{(k)}$\n",
    "\n",
    "2. If $G^{(k)}$ is positive definite ,\n",
    "\n",
    "   solve $G^{(k)}d^{(k)}$ = $-g^{(k)}$ for $d^{(k)}$\n",
    "\n",
    "   Else stop \n",
    "\n",
    "3. Check convergence $\\lVert g^{(k)}\\rVert\\: \\leq\\:\\epsilon_g$\n",
    "\n",
    "4. set $x^{(k+1)}\\:=\\:x^{(k)}+d^{(k)}$ for $\\alpha_k=1$\n",
    "\n",
    "5. set $k=k+1$ and go to 1\n",
    "\n",
    "Note that $x_{k+1}$ can be written as\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^{(k+1)}\\:=\\:x^{(k)}+[G^k]^{-1}g^k\n",
    "\\end{align*}\n",
    "$$\n",
    "Remark\n",
    "\n",
    "Basic newton's method doesn't involve a line search Thus because the quadratic model provides an estimate of a step to the minimum along the line $x_k+\\alpha_kd_k$, not just a search direction as with the steepest decent direction.\n",
    "\n",
    "If $G_k$ is positive definite and $x_k$ is not a stationary pt. $(g_k\\neq 0)$ then the Newton direction $d_k = -G^{-1}_kg_k$ is the descent direction as follow\n",
    "$$\n",
    "\\begin{align*}\n",
    "g^T_kd_k =\\:& g^T_k(-G^{-1}_kg_k)\\\\\n",
    "=\\:& -(g^T_kG^{-1}_kg_k)\\:<\\:0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\underline{Theorem}\\;\\;$ Let $x^*$ be a minimizer of $f(\\overline{x})$ at which $G^*$ is positive definite, If $x_0$ is sufficient close to $x^*$ then Newton's method is well define for all $k$ and it converge to $x^*$ at a quadratic rate \n",
    "\n",
    "  If $x^*$ is a local minimizer where $G^*$ is singular then Newton's method converge the rate of convergence is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. (newton's method on a quadratic function)\n",
    "\n",
    "Min $f(\\bar{x}) = x_1^2+10x_2^2$, starting from $x_1= \\begin{bmatrix} 1 \\\\ \\frac{1}{10} \\end{bmatrix}$,\n",
    "Using Newton's method:\n",
    "\n",
    "The gradient and Hessian of $f$ are\n",
    "$$\n",
    "\\begin{equation*}\n",
    "g(\\bar{x}) = \n",
    "\\begin{bmatrix}\n",
    "    2x_1 \\\\\n",
    "    20x_2 \\\\\n",
    "\\end{bmatrix}\\;\n",
    "and\n",
    "\\;\\;G(\\bar{x}) = \n",
    "\\begin{bmatrix}\n",
    "    2 & 0 \\\\\n",
    "    0 & 20 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "As $G(\\bar{x})$ is positive definite , $f$ is a strictly convex quadratic function, then\n",
    "$$\n",
    "\\begin{equation*}\n",
    "g^{(1)} = \n",
    "\\begin{bmatrix}\n",
    "    2(1) \\\\\n",
    "    20x\\frac{1}{10} \\\\\n",
    "\\end{bmatrix}\n",
    "\\:= \n",
    "\\begin{bmatrix}\n",
    "    2\\\\\n",
    "    2\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The newton's dirction  is the solution of\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Gd^{(1)}\\:&=-g^{(1)} \\:or \\: d^{(1)}\\:=-G^{-1}g^{(1)}\\\\\n",
    "d^{(1)}\\:&=-\n",
    "\\begin{bmatrix}\n",
    "    \\frac{20}{40} & 0\\\\\n",
    "    0 & \\frac{20}{40}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    2\\\\\n",
    "    2\\\\\n",
    "\\end{bmatrix}\n",
    "=-\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    \\frac{1}{10}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "    -1\\\\\n",
    "    -\\frac{1}{10}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "Hence,x^{(2)}\\:&=X^{(1)}+d^{(1)}=\n",
    "\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    \\frac{1}{10}\\\\\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "    -1\\\\\n",
    "    -\\frac{1}{10}\\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "\\end{bmatrix}=\n",
    "x^*\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $f(x^{(2)})=0$ and $g^{(2)}=\\nabla f(x^{(2)})=0$\n",
    "\n",
    "This is the exact minimizer which Newton's method found in 1 iteration as $f$ is a strictly convex quadratic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.4 Modified Newton's method<br>\n",
    "\n",
    "$\\qquad$The basic newton method is no line search the search direction $d^{(k)}=G^{(k)-1}g^{(k)}$ is not defined $G^{(k)}$ is singular. If $G^{(k)}$ is nonsingular but not be positive definite, solving $G^{(k)}d^{(k)}\\:=\\:-g^{(k)}$ is no guarantee that $f(x^{k+1})<f(x^{k})$\n",
    "  \n",
    "The major difficulty with Newton's method arise where $G^{(k)}$ is not positive definite\n",
    "  \n",
    "If $G^{(k)}$ is not positive definite , $d^{(k)}$ may not be decent direction. None of these problem arise if $G^{(k)}$ is positive \n",
    "definite\n",
    "\n",
    "One modification of Newton's method suggested to guarantee that the search direction are descent direction is to replace $G^{(k)}d^{(k)}= -g^{(k)}$ by\n",
    "$$\n",
    "\\begin{align*}\n",
    "(G^{(k)}+\\upsilon^{(k)}I)d^{(k)}\\:=\\:-g^{(k)}\n",
    "\\end{align*}\n",
    "$$\n",
    "Where $\\upsilon \\geq 0$ is choosen and $G^{(k)}+\\upsilon^{(k)}I$ is positive definite.\n",
    "\n",
    "We can choose $\\upsilon^{(k)}$ large enough so that $G^{(k)}+\\upsilon^{(k)}I$ is positive definite and so $d^{(k)}$ is a descent direction with the directional derivative\n",
    "$$\n",
    "\\begin{align*}\n",
    "g^{(k)T}d^{(k)}\\:=\\:-g^{(k)T}(G^{(k)})+\\upsilon^{(k)}I)^{-1}g^{(k)}\\:<0\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If $x^{(k)}\\rightarrow x^*$ where $G^*$ is positive definite , then the method should be $\\upsilon^{(k)}=0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
